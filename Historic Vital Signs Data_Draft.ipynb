{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d78a3b1",
   "metadata": {},
   "source": [
    "# Historic Vital Signs Data \n",
    "\n",
    "\n",
    "After a little bit of manual cleaning, I got the historic data (2000-2010) for many indicators into a common format, stored in several different excel files with several sheets each. Next step is to compile everything into a dataframe that's easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa0cfeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18412/2202643451.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[1;31m# for identifying which files to read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os # for identifying which files to read\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25bf5f",
   "metadata": {},
   "source": [
    "### Testing how to clean / compile the historic indicators from excel files - SKIP \n",
    "\n",
    "Starting with just testing things out.\n",
    "\n",
    "\n",
    "Goal is to write some functions to do this stuff quickly, but first let me see how things go with a single excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba23652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file names - only read the \"clean\" data\n",
    "filenames = os.listdir(\"Vital-Signs-10-Data-Tables/Vital Signs 10 Data Tables\")\n",
    "cleanfiles = [filename for filename in filenames if \"clean\" in filename]\n",
    "cleanpaths = [\"Vital-Signs-10-Data-Tables/Vital Signs 10 Data Tables/\" + filename for filename in cleanfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12487ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with one file\n",
    "test = pd.ExcelFile(cleanpaths[0])\n",
    "test.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_to_df_map = {}\n",
    "for sheet_name in test.sheet_names:\n",
    "    sheet_to_df_map[sheet_name] = test.parse(sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_female = sheet_to_df_map[\"femaleXX\"]\n",
    "pop_male = sheet_to_df_map[\"maleXX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5359f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_female.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b220e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melt example\n",
    "pop_female_melted = pop_female.melt(id_vars = \"Community Statistical Area (CSA)\",\n",
    "                         var_name = \"year\",\n",
    "                         value_name = \"femaleXX\")\\\n",
    ".rename(columns={\"Community Statistical Area (CSA)\":\"CSA\"})\n",
    "pop_female_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8c6a2",
   "metadata": {},
   "source": [
    "### Function to read / compile a single excel file into a dataframe  - SKIP \n",
    "\n",
    "Just load the previously compiled CSV now that I've already put it all together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f08704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_sheets(excel_path):\n",
    "    # read every sheet of the excel, store as a dicitonary of dataframes\n",
    "    xl = pd.ExcelFile(excel_path)\n",
    "    sheet_map = {}\n",
    "    for sheet_name in xl.sheet_names:\n",
    "        sheet_map[sheet_name] = xl.parse(sheet_name)\n",
    "        # standardize one particular column name\n",
    "        sheet_map[sheet_name] = sheet_map[sheet_name].rename(columns = {\"2006-2010\": \"2006 - 2010\",\n",
    "                                                                       \"2006- 2010\": \"2006 - 2010\"})\n",
    "    # turn dictionary of dataframes into a single dataframe\n",
    "    df = pd.concat(sheet_map).reset_index()\\\n",
    "    .rename(columns = {\"level_0\":\"indicator\", \"Community Statistical Area (CSA)\":\"CSA\"})\\\n",
    "    .drop(['level_1'],axis = 1)\n",
    "    # set indicator category based on file name \n",
    "    df[\"indicator_category\"] = str(excel_path[65:-5])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile all indicators across all clean historic excel files\n",
    "results = [compile_sheets(path) for path in cleanpaths]\n",
    "historic_indicators = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a250c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns\n",
    "historic_indicators = historic_indicators[['CSA',\n",
    "                                         'indicator',\n",
    "                                         'indicator_category',\n",
    "                                         2000,\n",
    "                                         2001,\n",
    "                                         2002,\n",
    "                                         2003,\n",
    "                                         2004,\n",
    "                                         2005,\n",
    "                                         2006,\n",
    "                                         2007,\n",
    "                                         2008,\n",
    "                                         2009,\n",
    "                                         2010,\n",
    "                                         '2006 - 2010'\n",
    "                                        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b762dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd32cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv so we can just load this file going forward \n",
    "historic_indicators.to_csv(\"precompiled_historic_indicators.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1286f",
   "metadata": {},
   "source": [
    "# Vital Signs from 2010 onward (attempt 1 - *failed*)\n",
    "\n",
    "Making use of BNIA's API\n",
    "\n",
    "Update 1/21 - don't use this section. Keeping it for posterity, but something about my function was causing the kernel to die whenever I made multiple API pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988900a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of previously created functions\n",
    "def getGDFfromURL(url, layer=0):\n",
    "    #GDF stands for GeoDataFrame; this is the innermost function called by getGDF\n",
    "    tail = \"/\"+str(layer)+\"/query?where=1%3D1&outFields=*&outSR=4326&f=json\" #worked this out\n",
    "    url+=tail\n",
    "    print(url)\n",
    "    gdf = gpd.read_file(url) #GeoPandas has a built in function to read APIs given right URL\n",
    "    return gdf\n",
    "\n",
    "def getGDF(shortname, level=0):\n",
    "    #This is outermost function called by user; it calls getGDFfromURL\n",
    "    url = api_df.loc[shortname, \"API\"]\n",
    "    return getGDFfromURL(url, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in info about the available indicators \n",
    "api_df = pd.read_csv(\"VS-Indicator-APIs.csv\")\n",
    "api_df.set_index(\"ShortName\", inplace=True) #Makes the dataframe index the shortname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a few indicators to loop through while testing\n",
    "indicator_sample = [\"paaXX\",\"pwhiteXX\", \"phispXX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07d2c5",
   "metadata": {},
   "source": [
    "#### Reformatting 1 indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reformatting 1 indicator \n",
    "paaXX_df = getGDF(\"paaXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99653f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for indicator \n",
    "#paaXX_df[\"indicator\"] = \"paaXX\"\n",
    "# remove object ID \n",
    "# change colnames to years (by index, but needs to be sensitive to specific year)\n",
    "# melt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42385dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "paaXX_df = paaXX_df.drop([\"OBJECTID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac186fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "['20' + i[-2:] for i in paaXX_df.columns[1:-3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46838f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of years to use as column names\n",
    "yrs = [int('20' + i[-2:]) for i in paaXX_df.columns[1:-3]]\n",
    "yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d903d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace column names\n",
    "for i in range(0,len(yrs)):\n",
    "    paaXX_df.columns.values[1+i] = yrs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the index variables? \n",
    "z = [i for i in paaXX_df.columns[-3:]]\n",
    "z.insert(0,paaXX_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b28a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966fa019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt \n",
    "paaXX_df.melt(id_vars = z, \n",
    "             var_name = \"year\", \n",
    "             value_name = \"paaXX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c9244",
   "metadata": {},
   "source": [
    "#### Try to loop and append a few dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEEMS TO WORK, BUT KERNAL KEEPS DYING! \n",
    "# function draft space \n",
    "def compile_post10_indicators_draft(indicator_list):\n",
    "    ### first indicator (retain CSA and geometry fields)\n",
    "    # get geopanadas dataframe from API\n",
    "    first_result = getGDF(indicator_list[0])\n",
    "    # drop object ID column \n",
    "    first_result = first_result.drop([\"OBJECTID\"], axis = 1)\n",
    "    # get list of years (columns) present in the data\n",
    "    first_yrs = [int('20' + i[-2:]) for i in first_result.columns[1:-3]]\n",
    "    # replace existing column names with actual year values \n",
    "    for i in range(0,len(first_yrs)):\n",
    "        first_result.columns.values[1+i] = first_yrs[i]\n",
    "    # melt (pivot longer) to a final dataframe \n",
    "    first_index_vars = [i for i in first_result.columns[-3:]]\n",
    "    first_index_vars.insert(0, first_result.columns[0])\n",
    "    df = first_result.melt(id_vars = first_index_vars, \n",
    "                                var_name = \"year\", \n",
    "                                value_name = indicator_list[0])\n",
    "    ### loop through this process for other indicators (and remove the geometry), and full join \n",
    "    for indicator in indicator_list[1:]:\n",
    "        result = getGDF(indicator).drop([\"OBJECTID\"], axis = 1)\n",
    "        yrs = [int('20' + i[-2:]) for i in result.columns[1:-3]]\n",
    "        for i in range(0,len(yrs)):\n",
    "            result.columns.values[1+i] = yrs[i]\n",
    "        index_vars = [i for i in result.columns[-3:]]\n",
    "        index_vars.insert(0, result.columns[0])\n",
    "        df_indicator = result.melt(id_vars = index_vars, \n",
    "                                   var_name = \"year\", \n",
    "                                   value_name = indicator)\n",
    "        df_indicator = df_indicator.iloc[:,[0,4,5]]  # drops the geometry columns \n",
    "        df = pd.merge(df, df_indicator, how = \"outer\", on = \"CSA2010\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de102e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying again. but this time, built for a single indicator \n",
    "def grab_reformat_indicator(indicator):\n",
    "    result = getGDF(indicator)\n",
    "    result = result.drop([\"OBJECTID\"], axis = 1)\n",
    "    yrs = [int('20' + i[-2:]) for i in result.columns[1:-3]]\n",
    "    for i in range(0,len(yrs)):\n",
    "        result.columns.values[1+i] = yrs[i]\n",
    "    index_vars = [i for i in result.columns[-3:]]\n",
    "    index_vars.insert(0, result.columns[0])\n",
    "    df = result.melt(id_vars = index_vars,\n",
    "                     var_name = \"year\",\n",
    "                     value_name = indicator)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfaba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_sample[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_post10_indicators(indicator_sample[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = compile_post10_indicators([\"paaXX\"])\n",
    "d2 = compile_post10_indicators([\"pwhiteXX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1274d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_2 = d2.iloc[:,[0,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(d1,d2_2, how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566ec7f",
   "metadata": {},
   "source": [
    "Things were going okay, but my kernel kept dying when I tried to run my function on multiple indicators.\n",
    "Rather than keep trying with this method, I went back to Colin's BNIA code and I'm going to make use of his 'collect' function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be92143",
   "metadata": {},
   "source": [
    "# Vital Signs from 2010 onwards, attempt 2 \n",
    "\n",
    "Making use of Colin's existing functions (I know they work, and I'm trying to avoid killing the kernel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of previously created functions\n",
    "def getGDFfromURL(url, layer=0):\n",
    "    #GDF stands for GeoDataFrame; this is the innermost function called by getGDF\n",
    "    tail = \"/\"+str(layer)+\"/query?where=1%3D1&outFields=*&outSR=4326&f=json\" #worked this out\n",
    "    url+=tail\n",
    "    print(url)\n",
    "    gdf = gpd.read_file(url) #GeoPandas has a built in function to read APIs given right URL\n",
    "    return gdf\n",
    "\n",
    "def getGDF(shortname, level=0):\n",
    "    #This is outermost function called by user; it calls getGDFfromURL\n",
    "    url = api_df.loc[shortname, \"API\"]\n",
    "    return getGDFfromURL(url, level)\n",
    "\n",
    "def getCollect(check_list):\n",
    "    #This function collects all the target GDFs and puts into collection\n",
    "    collect=[]\n",
    "    for shortname in check_list:\n",
    "        gdf=getGDF(shortname)\n",
    "        collect.append(gdf)    \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba6094",
   "metadata": {},
   "source": [
    "Rather than pull every single indicator, I've pared down the list a bit to remove indicators that aren't usable or wouldn't add anything particularly interesting to the analysis. Some indicators weren't very useful because the metric was collected by zip code or because the data was otherwise very sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read list of indicators \n",
    "api_df = pd.read_csv(\"VS-Indicator-APIs_EF.csv\") # new version - I've labeled which API calls to make under 'pull'\n",
    "#api_df.set_index(\"ShortName\", inplace=True) \n",
    "api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many API calls are we making? \n",
    "sum(api_df.pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### API calls \n",
    "# get list of shortnames indicators to pull \n",
    "indicator_list = list(api_df[api_df.pull == 1].index)\n",
    "# collect (first 50)\n",
    "collect1 = getCollect(indicator_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6c6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split up in case of errors along the way\n",
    "collect2 = getCollect(indicator_list[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect3 = getCollect(indicator_list[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn them into dataframes and append \n",
    "df1 = pd.concat(collect1)\n",
    "df2 = pd.concat(collect2)\n",
    "df3 = pd.concat(collect3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0184978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe of vital signs from 2010 onward (from 137 selected indicators)\n",
    "vs = df1.append(df2).append(df3)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the append worked properly, and we have the right number of rows \n",
    "df1.shape[0] + df2.shape[0] + df3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv (temporary file, to avoid repulling the data)\n",
    "df1.to_csv(\"modern_vital_signs_raw_1.csv\", index = False)\n",
    "df2.to_csv(\"modern_vital_signs_raw_2.csv\", index = False)\n",
    "df3.to_csv(\"modern_vital_signs_raw_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672c4c2",
   "metadata": {},
   "source": [
    "#### Reformat (modern) Data \n",
    "\n",
    "- Pivot: create a new column for indicator name, change column names from indicator + year to just year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22291187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_indicator(collect_object):\n",
    "    collect_object = collect_object.iloc[: , 1:] # drop the first column, which is object ID \n",
    "    yrs = ['20' + i[-2:] for i in collect_object.columns[1:-3]]\n",
    "    indicator = collect_object.columns[2][:-2]\n",
    "    for i in range(0,len(yrs)):\n",
    "        collect_object.columns.values[1+i] = yrs[i]\n",
    "    collect_object[\"indicator\"] = indicator\n",
    "    collect_object = collect_object.reset_index()\n",
    "    return collect_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9524384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference from last cell: the 'if' statement for objectID \n",
    "def reformat_indicator_2(collect_object):\n",
    "    if \"OBJECTID\" in collect_object.columns:\n",
    "        collect_object = collect_object.drop([\"OBJECTID\"], axis = 1)\n",
    "    yrs = ['20' + i[-2:] for i in collect_object.columns[1:-3]]\n",
    "    indicator = collect_object.columns[2][:-2]\n",
    "    for i in range(0,len(yrs)):\n",
    "        collect_object.columns.values[1+i] = yrs[i]\n",
    "    collect_object[\"indicator\"] = indicator\n",
    "    collect_object = collect_object.reset_index()\n",
    "    return collect_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one giant collection: \n",
    "full_collection = collect1.copy()\n",
    "[full_collection.append(c) for c in collect2]\n",
    "[full_collection.append(c) for c in collect3]\n",
    "len(full_collection) # now the correct length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba408e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for all of collect 1 (which is now everything) \n",
    "# COMMENTED OUT, KERNEL WAS DYING \n",
    "# reformatted_dfs = []\n",
    "# for c in full_collection: \n",
    "#     data = reformat_indicator(c).reset_index(drop = True)\n",
    "#     reformatted_dfs.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes out of collections \n",
    "### KERNEL KEEPS DYING! I'll go ahead and split this into many different files\n",
    "reformatted_dfs_1 = []\n",
    "for c in collect1[:25]: \n",
    "    reformatted_dfs_1.append(reformat_indicator(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab91805",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_1 = pd.concat(reformatted_dfs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de740ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_1.to_csv(\"modern_vital_signs_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5051b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_dfs_2 = []\n",
    "for c in collect2: \n",
    "    data = reformat_indicator(c).reset_index()\n",
    "    reformatted_dfs_2.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_dfs_3 = []\n",
    "for c in collect3: \n",
    "    data = reformat_indicator(c).reset_index()\n",
    "    reformatted_dfs_3.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bf205",
   "metadata": {},
   "source": [
    "### Kernel keeps dying! Work around approach.... \n",
    "\n",
    "\n",
    "Reading the previously saved data, even though it's in a terrible format. From there: \n",
    "- melt so that we have columns for neighborhood, geometry, and the metric itself. each indicator/year column will become a new row \n",
    "- add new column for year, based on indicator \n",
    "- add new column for indicator (agnostic of year) \n",
    "- drop indicator/year column \n",
    "- re-save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data files for modern indicators \n",
    "mvs1 = pd.read_csv(\"modern_vital_signs_raw_1.csv\")\n",
    "mvs2 = pd.read_csv(\"modern_vital_signs_raw_2.csv\")\n",
    "mvs3 = pd.read_csv(\"modern_vital_signs_raw_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ce548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## reformat / melt \n",
    "# msv1 \n",
    "objectid_cols = [col for col in mvs1.columns if \"OBJECTID\" in col]\n",
    "\n",
    "mvs1 = mvs1.drop(objectid_cols, axis = 1)\n",
    "# drop geometry as well; it's causing some problems\n",
    "mvs1 = mvs1.drop(['Shape__Area', 'Shape__Length', \"geometry\"], axis = 1)\n",
    "index_cols = [\"CSA2010\"]\n",
    "mvs_df_1 = mvs1.melt(id_vars = index_cols,\n",
    "                     var_name = \"year-indicator\",\n",
    "                     value_name = \"value\")\n",
    "# msv2 \n",
    "objectid_cols = [col for col in mvs2.columns if \"OBJECTID\" in col]\n",
    "mvs2 = mvs2.drop(objectid_cols, axis = 1)\n",
    "mvs2 = mvs2.drop(['Shape__Area', 'Shape__Length', \"geometry\"], axis = 1)\n",
    "mvs_df_2 = mvs2.melt(id_vars = index_cols,\n",
    "                     var_name = \"year-indicator\",\n",
    "                     value_name = \"value\")\n",
    "# msv3\n",
    "objectid_cols = [col for col in mvs3.columns if \"OBJECTID\" in col]\n",
    "mvs3 = mvs3.drop(objectid_cols, axis = 1)\n",
    "mvs3 = mvs3.drop(['Shape__Area', 'Shape__Length', \"geometry\"], axis = 1)\n",
    "mvs_df_3 = mvs3.melt(id_vars = index_cols,\n",
    "                     var_name = \"year-indicator\",\n",
    "                     value_name = \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add column for year, based on indicator/year field \n",
    "mvs_df_1[\"year\"] = ['20' + i[-2:] for i in mvs_df_1[\"year-indicator\"]]\n",
    "mvs_df_2[\"year\"] = ['20' + i[-2:] for i in mvs_df_2[\"year-indicator\"]]\n",
    "mvs_df_3[\"year\"] = ['20' + i[-2:] for i in mvs_df_3[\"year-indicator\"]]\n",
    "## add column for indicator, based on indicator/year field \n",
    "mvs_df_1[\"indicator\"] = [i[:-2] for i in mvs_df_1[\"year-indicator\"]]\n",
    "mvs_df_2[\"indicator\"] = [i[:-2] for i in mvs_df_2[\"year-indicator\"]]\n",
    "mvs_df_3[\"indicator\"] = [i[:-2] for i in mvs_df_3[\"year-indicator\"]]\n",
    "# drop year-indicator field \n",
    "mvs_df_1 = mvs_df_1.drop([\"year-indicator\"], axis = 1)\n",
    "mvs_df_2 = mvs_df_2.drop([\"year-indicator\"], axis = 1)\n",
    "mvs_df_3 = mvs_df_3.drop([\"year-indicator\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# somehow ended up with a lot of duplicates because of NA values. Drop those \n",
    "mvs_df_1.dropna(subset = [\"value\"], inplace = True)\n",
    "mvs_df_2.dropna(subset = [\"value\"], inplace = True)\n",
    "mvs_df_3.dropna(subset = [\"value\"], inplace = True)\n",
    "# it also seems like there are some indicators where the API failed to pull data, resulting in NAs in CSA2010 \n",
    "mvs_df_1.dropna(subset = [\"CSA2010\"], inplace = True)\n",
    "mvs_df_2.dropna(subset = [\"CSA2010\"], inplace = True)\n",
    "mvs_df_3.dropna(subset = [\"CSA2010\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea99a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pivot \n",
    "index_cols_pivotlonger = [\"CSA2010\", \"indicator\"]\n",
    "mvs_pivot_1 = mvs_df_1.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()\n",
    "mvs_pivot_2 = mvs_df_2.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()\n",
    "mvs_pivot_3 = mvs_df_3.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b51921",
   "metadata": {},
   "outputs": [],
   "source": [
    "### small data fix - remove some placeholder rows from mvs3 \n",
    "mvs_df_3 = mvs_df_3.query(\"indicator != 'CSA2010'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export! \n",
    "mvs_pivot_1.to_csv(\"modern_vital_signs_pivot_1.csv\", index = False)\n",
    "mvs_pivot_2.to_csv(\"modern_vital_signs_pivot_2.csv\", index = False)\n",
    "mvs_pivot_3.to_csv(\"modern_vital_signs_pivot_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0fd94",
   "metadata": {},
   "source": [
    "#### random troubleshooting below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols_pivotlonger = mvs_df_1.columns[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols_pivotlonger = [i for i in index_cols_pivotlonger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c042c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols_pivotlonger.append(\"indicator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b074aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols_pivotlonger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pivot \n",
    "# mvs_pivot_1 = mvs_df_1.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()\n",
    "# mvs_pivot_2 = mvs_df_2.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()\n",
    "# mvs_pivot_3 = mvs_df_3.pivot(index = index_cols_pivotlonger,columns = \"year\", values = \"value\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export! \n",
    "# mvs_pivot_1.to_csv(\"modern_vital_signs_pivot_1.csv\", index = False)\n",
    "# mvs_pivot_2.to_csv(\"modern_vital_signs_pivot_2.csv\", index = False)\n",
    "# mvs_pivot_3.to_csv(\"modern_vital_signs_pivot_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5395a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk why there are duplicates, let's look for them \n",
    "groupby_cols = index_cols_pivotlonger.copy()\n",
    "groupby_cols.append(\"year\")\n",
    "grouped = mvs_df_1.groupby(groupby_cols).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b45dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat.loc[dat.lifeExp < 25]\n",
    "# dat.query('lifeExp < 25')\n",
    "viol = mvs_df_1.query(\"indicator == 'viol'\")\n",
    "viol.query(\"year == '2016'\").dropna(subset = [\"value\"], inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs_df_1.query(\"indicator == 'demper'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a685b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a bunch of NA's in CSA2010 for 'demper' - why? \n",
    "demper_cols =[col for col in mvs1.columns if 'demper' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs1[demper_cols] # I'm assuming the API failed for this indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs1[mvs1.CSA2010.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20094d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs2[mvs2.CSA2010.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs3[mvs3.CSA2010.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c4aaa",
   "metadata": {},
   "source": [
    "# Combining Historic & Modern Vital Sign Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### historic data \n",
    "# read from CSV \n",
    "historic_indicators = pd.read_csv(\"precompiled_historic_indicators.csv\")\n",
    "# pivot longer for join \n",
    "hvs = historic_indicators.melt(id_vars = [\"CSA\", \"indicator\", \"indicator_category\"], \n",
    "                        var_name = \"year\")\n",
    "# drop indicator category (will add it later so that it's uniform)\n",
    "hvs = hvs.drop([\"indicator_category\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### modern data \n",
    "# reshape (longer pivot makes more sense for union )\n",
    "mvs_df_1 = mvs_df_1.rename(columns = {\"CSA2010\":\"CSA\"})\n",
    "mvs_df_2 = mvs_df_2.rename(columns = {\"CSA2010\":\"CSA\"})\n",
    "mvs_df_3 = mvs_df_3.rename(columns = {\"CSA2010\":\"CSA\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395226",
   "metadata": {},
   "outputs": [],
   "source": [
    "### concatenate into one big dataframe \n",
    "vs = pd.concat([hvs, mvs_df_1,mvs_df_2,mvs_df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean some names (remove asterisk from CSA names)\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"*\",\"\", regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc590a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for posterity \n",
    "vs.to_csv(\"full_vital_signs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1828fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what years are available for each indicator? \n",
    "vs[[\"indicator\",\"year\"]].groupby([\"indicator\"])[\"year\"].apply(set).reset_index()\n",
    "# just from a glance, it looks like there are some duplicates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a642209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an info dataframe of indicator, years available, description, and category \n",
    "info = vs[[\"indicator\",\"year\"]].groupby([\"indicator\"])[\"year\"].apply(set).reset_index()\n",
    "# grab the info from the api DF\n",
    "indicator_desc = api_df.rename(columns = {\"Indicator\":\"indicator_description\",\"ShortName\":\"indicator\",\"Section\":\"category\"})[[\"indicator_description\",\"indicator\",\"category\"]]\n",
    "info = info.merge(indicator_desc)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5027ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "info.to_csv(\"indicator_info.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all the neighborhood names are uniform \n",
    "len(set(vs.CSA)) # there should be about 55, so there's cleanup to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(mvs_df_1.CSA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean some names \n",
    "# remove asterisk from CSA names\n",
    "# correct some spellings \n",
    "# unify some names that may be abbreviated\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"*\",\"\", regex = False)\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"Edmonson\",\"Edmondson\")\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"Falstaff\",\"Fallstaff\") # really not sure which is right, but BNIA uses Fallstaff in modern communications\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"Mt. Washington\",\"Mount Washington\")\n",
    "vs[\"CSA\"] = vs.CSA.str.replace(\"Mt. Winans\",\"Mount Winans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(vs.CSA)\n",
    "# things that need to get cleaned up: \n",
    "#* 2010 data using new 2010 CSA boundaries. CSA boundaries were modified slightly due to modifications in Census geographies from 2000 to 2010.\n",
    "# For more information, visit http://www.bniajfi.org.\n",
    "# NA = Data not available due to major modifications in Census geographies from 2000 to 2010. \n",
    "# nan \n",
    "# anything with a * \n",
    "# Edmonson Village vs Edmondson Village\n",
    "# Glen-Fallstaff vs. Glen-Falstaff\n",
    "# Jonestown/Oldtown vs. Oldtown / Middle East\n",
    "# Washington Village vs. Washington Village/Pigtown\n",
    "# Westport/Mount Winans/Lakeland vs Westport/Mt. Winans/Lakeland\n",
    "# Perkins/Middle East vs Oldtown/Middle East\n",
    "# 'Medfield/Hampden/Woodberry', vs 'Medfield/Hampden/Woodberry/Remington'\n",
    "#  'Mount Washington/Coldspring','Mt. Washington/Coldspring',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: check the api call for \"Hhsize\" \n",
    "# modern data \n",
    "# does not seem to deliver a year, which causes some weirdness in the data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaed4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
